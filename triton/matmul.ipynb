{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(x_ptr, y_ptr, z_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    # 获取线程的全局索引\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    # 边界检查\n",
    "    mask = offsets < n_elements\n",
    "    # 加载数据并计算\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    z = x + y\n",
    "    # 存储结果\n",
    "    tl.store(z_ptr + offsets, z, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# 测试\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# 生成随机矩阵\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1024\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Triton矩阵乘法\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/anaconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "    # 输入矩阵的指针\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    # 矩阵维度\n",
    "    M, N, K,\n",
    "    # 矩阵的步长（stride）\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    # 分块大小（编译期常量）\n",
    "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "):\n",
    "    # 获取当前程序的ID（对应输出矩阵的块）\n",
    "    pid = tl.program_id(axis=0)\n",
    "    # 计算输出块的坐标\n",
    "    pid_m = pid // (N // BLOCK_SIZE_N)\n",
    "    pid_n = pid % (N // BLOCK_SIZE_N)\n",
    "\n",
    "    # 定义输入块的偏移量\n",
    "    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "\n",
    "    # 计算指针的偏移\n",
    "    a_ptr = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n",
    "    b_ptr = b_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n",
    "\n",
    "    # 初始化累加器\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    # 循环加载K维度的块并计算\n",
    "    for k in range(0, K, BLOCK_SIZE_K):\n",
    "        # 加载A和B的块到寄存器\n",
    "        a = tl.load(a_ptr, mask=offs_k[None, :] + k < K, other=0.0)\n",
    "        b = tl.load(b_ptr, mask=offs_k[:, None] + k < K, other=0.0)\n",
    "        # 矩阵乘法累加\n",
    "        accumulator += tl.dot(a, b)\n",
    "        # 移动指针到下一个K块\n",
    "        a_ptr += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptr += BLOCK_SIZE_K * stride_bk\n",
    "\n",
    "    # 加载输出块的偏移\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptr = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n",
    "    # 存储结果到输出矩阵\n",
    "    tl.store(c_ptr, accumulator, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n",
    "\n",
    "# 封装为Python函数\n",
    "def triton_matmul(a, b):\n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "    # 初始化输出矩阵\n",
    "    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n",
    "    # 定义分块大小（可根据GPU架构调整）\n",
    "    BLOCK_SIZE_M = BLOCK_SIZE_N = BLOCK_SIZE_K = 128\n",
    "    # 计算需要的程序数（输出块的数量）\n",
    "    grid = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n",
    "    # 启动Triton内核\n",
    "    matmul_kernel[grid](\n",
    "        a, b, c,\n",
    "        M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K\n",
    "    )\n",
    "    return c\n",
    "\n",
    "# 测试\n",
    "if __name__ == \"__main__\":\n",
    "    # 生成随机矩阵\n",
    "    a = torch.randn(1024, 512, device=\"cuda\", dtype=torch.float32)\n",
    "    b = torch.randn(512, 1024, device=\"cuda\", dtype=torch.float32)\n",
    "    # Triton矩阵乘法\n",
    "    c_triton = triton_matmul(a, b)\n",
    "    # PyTorch内置矩阵乘法\n",
    "    c_torch = torch.matmul(a, b)\n",
    "    # 验证结果正确性\n",
    "    print(\"结果误差:\", torch.max(torch.abs(c_triton - c_torch)))\n",
    "    # 性能对比\n",
    "    %timeit triton_matmul(a, b)\n",
    "    %timeit torch.matmul(a, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
