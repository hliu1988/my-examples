{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://docs.pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to `torch.compile`\n",
    "===============================\n",
    "\n",
    "**Author:** William Wen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.compile` is the new way to speed up your PyTorch code!\n",
    "`torch.compile` makes PyTorch code run faster by JIT-compiling PyTorch\n",
    "code into optimized kernels, while requiring minimal code changes.\n",
    "\n",
    "`torch.compile` accomplishes this by tracing through your Python code,\n",
    "looking for PyTorch operations. Code that is difficult to trace will\n",
    "result a **graph break**, which are lost optimization opportunities,\n",
    "rather than errors or silent incorrectness.\n",
    "\n",
    "`torch.compile` is available in PyTorch 2.0 and later.\n",
    "\n",
    "This introduction covers basic `torch.compile` usage and demonstrates\n",
    "the advantages of `torch.compile` over our previous PyTorch compiler\n",
    "solution, [TorchScript](https://pytorch.org/docs/stable/jit.html).\n",
    "\n",
    "For an end-to-end example on a real model, check out our [end-to-end\n",
    "torch.compile\n",
    "tutorial](https://pytorch.org/tutorials/intermediate/torch_compile_full_example.html).\n",
    "\n",
    "To troubleshoot issues and to gain a deeper understanding of how to\n",
    "apply `torch.compile` to your code, check out [the torch.compile\n",
    "programming\n",
    "model](https://docs.pytorch.org/docs/main/compile/programming_model.html).\n",
    "\n",
    "**Contents**\n",
    "\n",
    "::: {.contents local=\"\"}\n",
    ":::\n",
    "\n",
    "**Required pip dependencies for this tutorial**\n",
    "\n",
    "-   `torch >= 2.0`\n",
    "-   `numpy`\n",
    "-   `scipy`\n",
    "\n",
    "**System requirements** - A C++ compiler, such as `g++` - Python\n",
    "development package (`python-devel`/`python-dev`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Usage\n",
    "===========\n",
    "\n",
    "We turn on some logging to help us to see what `torch.compile` is doing\n",
    "under the hood in this tutorial. The following code will print out the\n",
    "PyTorch ops that `torch.compile` traced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch._logging.set_logs(graph_code=True)\n",
    "# 不起作用\n",
    "# Enable verbose logging to see compilation details\n",
    "# torch._dynamo.config.verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code] TRACED GRAPH\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]  ===== __compiled_fn_7_9931b1aa_c92a_4185_8547_d7e651d37223 =====\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]     def forward(self, L_x_: \"f32[3, 3][3, 1]cpu\", L_y_: \"f32[3, 3][3, 1]cpu\"):\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         l_x_ = L_x_\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         l_y_ = L_y_\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         \n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         # File: /tmp/ipykernel_197418/14723609.py:2 in foo, code: a = torch.sin(x)\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         a: \"f32[3, 3][3, 1]cpu\" = torch.sin(l_x_);  l_x_ = None\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         \n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         # File: /tmp/ipykernel_197418/14723609.py:3 in foo, code: b = torch.cos(y)\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         b: \"f32[3, 3][3, 1]cpu\" = torch.cos(l_y_);  l_y_ = None\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         \n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         # File: /tmp/ipykernel_197418/14723609.py:4 in foo, code: return a + b\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         add: \"f32[3, 3][3, 1]cpu\" = a + b;  a = b = None\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         return (add,)\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         \n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code] \n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code] TRACED GRAPH\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]  ===== __compiled_fn_9_d958ac78_777e_486b_8702_64f81927b0a2 =====\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]     def forward(self, s77: \"Sym(s17)\", s27: \"Sym(s27)\", L_x_: \"f32[s17, s27][s27, 1]cpu\", L_y_: \"f32[s17, s27][s27, 1]cpu\"):\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         l_x_ = L_x_\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         l_y_ = L_y_\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         \n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         # File: /tmp/ipykernel_197418/14723609.py:2 in foo, code: a = torch.sin(x)\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         a: \"f32[s17, s27][s27, 1]cpu\" = torch.sin(l_x_);  l_x_ = None\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         \n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         # File: /tmp/ipykernel_197418/14723609.py:3 in foo, code: b = torch.cos(y)\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         b: \"f32[s17, s27][s27, 1]cpu\" = torch.cos(l_y_);  l_y_ = None\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         \n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         # File: /tmp/ipykernel_197418/14723609.py:4 in foo, code: return a + b\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         add: \"f32[s17, s27][s27, 1]cpu\" = a + b;  a = b = None\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         return (add,)\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         \n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9546,  1.8503,  0.8476],\n",
      "        [-0.0234, -0.1408,  1.9060],\n",
      "        [-1.5669,  0.2384,  1.4532]])\n",
      "tensor([[ 0.0889,  0.9144,  1.0922,  0.3792],\n",
      "        [ 0.8120,  0.0853, -0.2555,  0.7229],\n",
      "        [ 0.2868,  0.0549,  0.6464,  0.5942],\n",
      "        [-0.0562,  0.0907, -0.3454,  0.6626]])\n"
     ]
    }
   ],
   "source": [
    "def foo(x, y):\n",
    "    a = torch.sin(x)\n",
    "    b = torch.cos(y)\n",
    "    return a + b\n",
    "\n",
    "\n",
    "opt_foo1 = torch.compile(foo)\n",
    "print(opt_foo1(torch.randn(3, 3), torch.randn(3, 3)))\n",
    "print(opt_foo1(torch.randn(4, 4), torch.randn(4, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6818, 1.7326, 0.1336],\n",
      "        [0.0587, 0.4029, 0.8624],\n",
      "        [0.7774, 0.9512, 0.2888]])\n",
      "tensor([[ 0.7467,  0.0485,  1.4156,  0.0289],\n",
      "        [ 0.9113,  0.9799,  1.4499,  0.6286],\n",
      "        [ 0.0259, -0.1759, -0.9992, -1.0882],\n",
      "        [ 0.1066,  0.9046, -0.4007,  1.6108]])\n"
     ]
    }
   ],
   "source": [
    "print(opt_foo1(torch.randn(3, 3), torch.randn(3, 3)))\n",
    "print(opt_foo1(torch.randn(4, 4), torch.randn(4, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.compile` is a decorator that takes an arbitrary Python function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code] TRACED GRAPH\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]  ===== __compiled_fn_9_5e62b208_6380_4342_b318_8c393cd6df6f =====\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]     def forward(self, L_x_: \"f32[3, 3][3, 1]cpu\", L_y_: \"f32[3, 3][3, 1]cpu\"):\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         l_x_ = L_x_\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         l_y_ = L_y_\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         \n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         # File: /tmp/ipykernel_179889/4220512634.py:3 in opt_foo2, code: a = torch.sin(x)\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         a: \"f32[3, 3][3, 1]cpu\" = torch.sin(l_x_);  l_x_ = None\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         \n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         # File: /tmp/ipykernel_179889/4220512634.py:4 in opt_foo2, code: b = torch.cos(y)\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         b: \"f32[3, 3][3, 1]cpu\" = torch.cos(l_y_);  l_y_ = None\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         \n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         # File: /tmp/ipykernel_179889/4220512634.py:5 in opt_foo2, code: return a + b\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         add: \"f32[3, 3][3, 1]cpu\" = a + b;  a = b = None\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         return (add,)\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         \n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0671,  0.8626,  1.3965],\n",
      "        [ 1.4243,  0.3922,  0.3412],\n",
      "        [-0.9585, -1.1444, -0.2240]])\n"
     ]
    }
   ],
   "source": [
    "@torch.compile\n",
    "def opt_foo2(x, y):\n",
    "    a = torch.sin(x)\n",
    "    b = torch.cos(y)\n",
    "    return a + b\n",
    "\n",
    "print(opt_foo2(torch.randn(3, 3), torch.randn(3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.compile` is applied recursively, so nested function calls within\n",
    "the top-level compiled function will also be compiled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code] TRACED GRAPH\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]  ===== __compiled_fn_13_6f0e438d_f42f_4d18_af2a_f6be1b9424d2 =====\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]     def forward(self, L_x_: \"f32[3, 3][3, 1]cpu\", L_y_: \"f32[3, 3][3, 1]cpu\"):\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         l_x_ = L_x_\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         l_y_ = L_y_\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         \n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         # File: /tmp/ipykernel_179889/2199486826.py:2 in inner, code: return torch.sin(x)\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         a: \"f32[3, 3][3, 1]cpu\" = torch.sin(l_x_);  l_x_ = None\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         \n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         # File: /tmp/ipykernel_179889/2199486826.py:8 in outer, code: b = torch.cos(y)\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         b: \"f32[3, 3][3, 1]cpu\" = torch.cos(l_y_);  l_y_ = None\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         \n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         # File: /tmp/ipykernel_179889/2199486826.py:9 in outer, code: return a + b\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         add: \"f32[3, 3][3, 1]cpu\" = a + b;  a = b = None\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         return (add,)\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         \n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7620,  0.5823, -0.2230],\n",
      "        [ 0.4700,  1.6073,  0.1784],\n",
      "        [ 1.5852,  0.1588,  0.6764]])\n"
     ]
    }
   ],
   "source": [
    "def inner(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "\n",
    "@torch.compile\n",
    "def outer(x, y):\n",
    "    a = inner(x)\n",
    "    b = torch.cos(y)\n",
    "    return a + b\n",
    "\n",
    "\n",
    "print(outer(torch.randn(3, 3), torch.randn(3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also optimize `torch.nn.Module` instances by either calling its\n",
    "`.compile()` method or by directly `torch.compile`-ing the module. This\n",
    "is equivalent to `torch.compile`-ing the module\\'s `__call__` method\n",
    "(which indirectly calls `forward`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code] TRACED GRAPH\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]  ===== __compiled_fn_27_9c390f3e_399a_4717_9ceb_a8f78871a303 =====\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]     def forward(self, L_self_modules_lin_parameters_weight_: \"f32[3, 3][3, 1]cpu\", L_self_modules_lin_parameters_bias_: \"f32[3][1]cpu\", L_x_: \"f32[3, 3][3, 1]cpu\"):\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         l_self_modules_lin_parameters_weight_ = L_self_modules_lin_parameters_weight_\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         l_self_modules_lin_parameters_bias_ = L_self_modules_lin_parameters_bias_\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         l_x_ = L_x_\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         \n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         # File: /tmp/ipykernel_179889/3046762702.py:7 in forward, code: return torch.nn.functional.relu(self.lin(x))\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         linear: \"f32[3, 3][3, 1]cpu\" = torch._C._nn.linear(l_x_, l_self_modules_lin_parameters_weight_, l_self_modules_lin_parameters_bias_);  l_x_ = l_self_modules_lin_parameters_weight_ = l_self_modules_lin_parameters_bias_ = None\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         relu: \"f32[3, 3][3, 1]cpu\" = torch.nn.functional.relu(linear);  linear = None\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         return (relu,)\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         \n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7187, 0.9459, 0.6886],\n",
      "        [0.0000, 0.5362, 0.3013],\n",
      "        [0.0000, 0.4415, 0.0248]], grad_fn=<CompiledFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = torch.nn.Linear(3, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.relu(self.lin(x))\n",
    "\n",
    "mod2 = MyModule()\n",
    "mod2 = torch.compile(mod2)\n",
    "print(mod2(torch.randn(3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0596, 0.1275],\n",
      "        [0.0000, 0.7664, 0.0000],\n",
      "        [0.1984, 0.2656, 0.0000]], grad_fn=<CompiledFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 一个模型只会被编译一次，重复调用 compile 方法不会重新编译模型\n",
    "# 下面的代码与上面的效果是一样的\n",
    "mod1 = MyModule()\n",
    "mod1.compile()\n",
    "print(mod1(torch.randn(3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrating Speedups\n",
    "======================\n",
    "\n",
    "Now let\\'s demonstrate how `torch.compile` speeds up a simple PyTorch\n",
    "example. For a demonstration on a more complex model, see our\n",
    "[end-to-end torch.compile\n",
    "tutorial](https://pytorch.org/tutorials/intermediate/torch_compile_full_example.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code] TRACED GRAPH\n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code]  ===== __compiled_fn_17_71b2bb6f_a5b0_4011_bb46_6950def1a0fb =====\n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code]     def forward(self, L_x_: \"f32[4096, 4096][4096, 1]cuda:0\"):\n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code]         l_x_ = L_x_\n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code]         \n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code]         # File: /tmp/ipykernel_197418/3035428173.py:2 in foo3, code: y = x + 1\n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code]         y: \"f32[4096, 4096][4096, 1]cuda:0\" = l_x_ + 1;  l_x_ = None\n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code]         \n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code]         # File: /tmp/ipykernel_197418/3035428173.py:3 in foo3, code: z = torch.nn.functional.relu(y)\n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code]         z: \"f32[4096, 4096][4096, 1]cuda:0\" = torch.nn.functional.relu(y);  y = None\n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code]         \n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code]         # File: /tmp/ipykernel_197418/3035428173.py:4 in foo3, code: u = z * 2\n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code]         u: \"f32[4096, 4096][4096, 1]cuda:0\" = z * 2;  z = None\n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code]         return (u,)\n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code]         \n",
      "V0125 15:22:03.694000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [7/0] [__graph_code] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile: 0.02560748863220215\n"
     ]
    }
   ],
   "source": [
    "def foo3(x):\n",
    "    y = x + 1\n",
    "    z = torch.nn.functional.relu(y)\n",
    "    u = z * 2\n",
    "    return u\n",
    "\n",
    "\n",
    "opt_foo3 = torch.compile(foo3)\n",
    "\n",
    "\n",
    "# Returns the result of running `fn()` and the time it took for `fn()` to run,\n",
    "# in seconds. We use CUDA events and synchronization for the most accurate\n",
    "# measurements.\n",
    "def timed(fn):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end) / 1000\n",
    "\n",
    "# Warmup\n",
    "inp = torch.randn(4096, 4096).cuda()\n",
    "print(\"compile:\", timed(lambda: opt_foo3(inp))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile: 0.0004937280118465423\n",
      "eager: 0.0008568959832191467\n"
     ]
    }
   ],
   "source": [
    "inp = torch.randn(4096, 4096).cuda()\n",
    "print(\"compile:\", timed(lambda: opt_foo3(inp))[1])\n",
    "print(\"eager:\", timed(lambda: foo3(inp))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `torch.compile` appears to take a lot longer to complete\n",
    "compared to eager. This is because `torch.compile` takes extra time to\n",
    "compile the model on the first few executions. `torch.compile` re-uses\n",
    "compiled code whever possible, so if we run our optimized model several\n",
    "more times, we should see a significant improvement compared to eager.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turn off logging for now to prevent spam\n",
    "torch._logging.set_logs(graph_code=False)\n",
    "\n",
    "eager_times = []\n",
    "for i in range(10):\n",
    "    _, eager_time = timed(lambda: foo3(inp))\n",
    "    eager_times.append(eager_time)\n",
    "    print(f\"eager time {i}: {eager_time}\")\n",
    "print(\"~\" * 10)\n",
    "\n",
    "compile_times = []\n",
    "for i in range(10):\n",
    "    _, compile_time = timed(lambda: opt_foo3(inp))\n",
    "    compile_times.append(compile_time)\n",
    "    print(f\"compile time {i}: {compile_time}\")\n",
    "print(\"~\" * 10)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "eager_med = np.median(eager_times)\n",
    "compile_med = np.median(compile_times)\n",
    "speedup = eager_med / compile_med\n",
    "assert speedup > 1\n",
    "print(\n",
    "    f\"(eval) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\"\n",
    ")\n",
    "print(\"~\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, we can see that running our model with `torch.compile`\n",
    "results in a significant speedup. Speedup mainly comes from reducing\n",
    "Python overhead and GPU read/writes, and so the observed speedup may\n",
    "vary on factors such as model architecture and batch size. For example,\n",
    "if a model\\'s architecture is simple and the amount of data is large,\n",
    "then the bottleneck would be GPU compute and the observed speedup may be\n",
    "less significant.\n",
    "\n",
    "To see speedups on a real model, check out our [end-to-end torch.compile\n",
    "tutorial](https://pytorch.org/tutorials/intermediate/torch_compile_full_example.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benefits over TorchScript\n",
    "=========================\n",
    "\n",
    "Why should we use `torch.compile` over TorchScript? Primarily, the\n",
    "advantage of `torch.compile` lies in its ability to handle arbitrary\n",
    "Python code with minimal changes to existing code.\n",
    "\n",
    "Compare to TorchScript, which has a tracing mode (`torch.jit.trace`) and\n",
    "a scripting mode (`torch.jit.script`). Tracing mode is susceptible to\n",
    "silent incorrectness, while scripting mode requires significant code\n",
    "changes and will raise errors on unsupported Python code.\n",
    "\n",
    "For example, TorchScript tracing silently fails on data-dependent\n",
    "control flow (the `if x.sum() < 0:` line below) because only the actual\n",
    "control flow path is traced. In comparison, `torch.compile` is able to\n",
    "correctly handle it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f1(x, y):\n",
    "    if x.sum() < 0:\n",
    "        return -y\n",
    "    return y\n",
    "\n",
    "\n",
    "# Test that `fn1` and `fn2` return the same result, given the same arguments `args`.\n",
    "def test_fns(fn1, fn2, args):\n",
    "    out1 = fn1(*args)\n",
    "    out2 = fn2(*args)\n",
    "    return torch.allclose(out1, out2)\n",
    "\n",
    "\n",
    "inp1 = torch.randn(5, 5)\n",
    "inp2 = torch.randn(5, 5)\n",
    "\n",
    "traced_f1 = torch.jit.trace(f1, (inp1, inp2))\n",
    "print(\"traced 1, 1:\", test_fns(f1, traced_f1, (inp1, inp2)))\n",
    "print(\"traced 1, 2:\", test_fns(f1, traced_f1, (-inp1, inp2)))\n",
    "\n",
    "compile_f1 = torch.compile(f1)\n",
    "print(\"compile 1, 1:\", test_fns(f1, compile_f1, (inp1, inp2)))\n",
    "print(\"compile 1, 2:\", test_fns(f1, compile_f1, (-inp1, inp2)))\n",
    "print(\"~\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchScript scripting can handle data-dependent control flow, but it can\n",
    "require major code changes and will raise errors when unsupported Python\n",
    "is used.\n",
    "\n",
    "In the example below, we forget TorchScript type annotations and we\n",
    "receive a TorchScript error because the input type for argument `y`, an\n",
    "`int`, does not match with the default argument type, `torch.Tensor`. In\n",
    "comparison, `torch.compile` works without requiring any type\n",
    "annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import traceback as tb\n",
    "\n",
    "torch._logging.set_logs(graph_code=True)\n",
    "\n",
    "\n",
    "def f2(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "inp1 = torch.randn(5, 5)\n",
    "inp2 = 3\n",
    "\n",
    "script_f2 = torch.jit.script(f2)\n",
    "try:\n",
    "    script_f2(inp1, inp2)\n",
    "except:\n",
    "    tb.print_exc()\n",
    "\n",
    "compile_f2 = torch.compile(f2)\n",
    "print(\"compile 2:\", test_fns(f2, compile_f2, (inp1, inp2)))\n",
    "print(\"~\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Breaks\n",
    "============\n",
    "\n",
    "The graph break is one of the most fundamental concepts within\n",
    "`torch.compile`. It allows `torch.compile` to handle arbitrary Python\n",
    "code by interrupting compilation, running the unsupported code, then\n",
    "resuming compilation. The term \\\"graph break\\\" comes from the fact that\n",
    "`torch.compile` attempts to capture and optimize the PyTorch operation\n",
    "graph. When unsupported Python code is encountered, then this graph must\n",
    "be \\\"broken\\\". Graph breaks result in lost optimization opportunities,\n",
    "which may still be undesirable, but this is better than silent\n",
    "incorrectness or a hard crash.\n",
    "\n",
    "Let\\'s look at a data-dependent control flow example to better see how\n",
    "graph breaks work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bar(a, b):\n",
    "    x = a / (torch.abs(a) + 1)\n",
    "    if b.sum() < 0:\n",
    "        b = b * -1\n",
    "    return x * b\n",
    "\n",
    "\n",
    "opt_bar = torch.compile(bar)\n",
    "inp1 = torch.ones(10)\n",
    "inp2 = torch.ones(10)\n",
    "opt_bar(inp1, inp2)\n",
    "opt_bar(inp1, -inp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time we run `bar`, we see that `torch.compile` traced 2 graphs\n",
    "corresponding to the following code (noting that `b.sum() < 0` is\n",
    "False):\n",
    "\n",
    "1.  `x = a / (torch.abs(a) + 1); b.sum()`\n",
    "2.  `return x * b`\n",
    "\n",
    "The second time we run `bar`, we take the other branch of the if\n",
    "statement and we get 1 traced graph corresponding to the code\n",
    "`b = b * -1; return x * b`. We do not see a graph of\n",
    "`x = a / (torch.abs(a) + 1)` outputted the second time since\n",
    "`torch.compile` cached this graph from the first run and re-used it.\n",
    "\n",
    "Let\\'s investigate by example how TorchDynamo would step through `bar`.\n",
    "If `b.sum() < 0`, then TorchDynamo would run graph 1, let Python\n",
    "determine the result of the conditional, then run graph 2. On the other\n",
    "hand, if `not b.sum() < 0`, then TorchDynamo would run graph 1, let\n",
    "Python determine the result of the conditional, then run graph 3.\n",
    "\n",
    "We can see all graph breaks by using\n",
    "`torch._logging.set_logs(graph_breaks=True)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reset to clear the torch.compile cache\n",
    "torch._dynamo.reset()\n",
    "opt_bar(inp1, inp2)\n",
    "opt_bar(inp1, -inp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to maximize speedup, graph breaks should be limited. We can\n",
    "force TorchDynamo to raise an error upon the first graph break\n",
    "encountered by using `fullgraph=True`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reset to clear the torch.compile cache\n",
    "torch._dynamo.reset()\n",
    "\n",
    "opt_bar_fullgraph = torch.compile(bar, fullgraph=True)\n",
    "try:\n",
    "    opt_bar_fullgraph(torch.randn(10), torch.randn(10))\n",
    "except:\n",
    "    tb.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example above, we can work around this graph break by replacing\n",
    "the if statement with a `torch.cond`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functorch.experimental.control_flow import cond\n",
    "\n",
    "\n",
    "@torch.compile(fullgraph=True)\n",
    "def bar_fixed(a, b):\n",
    "    x = a / (torch.abs(a) + 1)\n",
    "\n",
    "    def true_branch(y):\n",
    "        return y * -1\n",
    "\n",
    "    def false_branch(y):\n",
    "        # NOTE: torch.cond doesn't allow aliased outputs\n",
    "        return y.clone()\n",
    "\n",
    "    x = cond(b.sum() < 0, true_branch, false_branch, (b,))\n",
    "    return x * b\n",
    "\n",
    "\n",
    "bar_fixed(inp1, inp2)\n",
    "bar_fixed(inp1, -inp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to serialize graphs or to run graphs on different (i.e.\n",
    "Python-less) environments, consider using `torch.export` instead (from\n",
    "PyTorch 2.1+). One important restriction is that `torch.export` does not\n",
    "support graph breaks. Please check [the torch.export\n",
    "tutorial](https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html)\n",
    "for more details on `torch.export`.\n",
    "\n",
    "Check out our [section on graph breaks in the torch.compile programming\n",
    "model](https://docs.pytorch.org/docs/main/compile/programming_model.graph_breaks_index.html)\n",
    "for tips on how to work around graph breaks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troubleshooting\n",
    "===============\n",
    "\n",
    "Is `torch.compile` failing to speed up your model? Is compile time\n",
    "unreasonably long? Is your code recompiling excessively? Are you having\n",
    "difficulties dealing with graph breaks? Are you looking for tips on how\n",
    "to best use `torch.compile`? Or maybe you simply want to learn more\n",
    "about the inner workings of `torch.compile`?\n",
    "\n",
    "Check out [the torch.compile programming\n",
    "model](https://docs.pytorch.org/docs/main/compile/programming_model.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "==========\n",
    "\n",
    "In this tutorial, we introduced `torch.compile` by covering basic usage,\n",
    "demonstrating speedups over eager mode, comparing to TorchScript, and\n",
    "briefly describing graph breaks.\n",
    "\n",
    "For an end-to-end example on a real model, check out our [end-to-end\n",
    "torch.compile\n",
    "tutorial](https://pytorch.org/tutorials/intermediate/torch_compile_full_example.html).\n",
    "\n",
    "To troubleshoot issues and to gain a deeper understanding of how to\n",
    "apply `torch.compile` to your code, check out [the torch.compile\n",
    "programming\n",
    "model](https://docs.pytorch.org/docs/main/compile/programming_model.html).\n",
    "\n",
    "We hope that you will give `torch.compile` a try!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
