{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://docs.pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to `torch.compile`\n",
    "===============================\n",
    "\n",
    "**Author:** William Wen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.compile` is the new way to speed up your PyTorch code!\n",
    "`torch.compile` makes PyTorch code run faster by JIT-compiling PyTorch\n",
    "code into optimized kernels, while requiring minimal code changes.\n",
    "\n",
    "`torch.compile` accomplishes this by tracing through your Python code,\n",
    "looking for PyTorch operations. Code that is difficult to trace will\n",
    "result a **graph break**, which are lost optimization opportunities,\n",
    "rather than errors or silent incorrectness.\n",
    "\n",
    "`torch.compile` is available in PyTorch 2.0 and later.\n",
    "\n",
    "This introduction covers basic `torch.compile` usage and demonstrates\n",
    "the advantages of `torch.compile` over our previous PyTorch compiler\n",
    "solution, [TorchScript](https://pytorch.org/docs/stable/jit.html).\n",
    "\n",
    "For an end-to-end example on a real model, check out our [end-to-end\n",
    "torch.compile\n",
    "tutorial](https://pytorch.org/tutorials/intermediate/torch_compile_full_example.html).\n",
    "\n",
    "To troubleshoot issues and to gain a deeper understanding of how to\n",
    "apply `torch.compile` to your code, check out [the torch.compile\n",
    "programming\n",
    "model](https://docs.pytorch.org/docs/main/compile/programming_model.html).\n",
    "\n",
    "**Contents**\n",
    "\n",
    "::: {.contents local=\"\"}\n",
    ":::\n",
    "\n",
    "**Required pip dependencies for this tutorial**\n",
    "\n",
    "-   `torch >= 2.0`\n",
    "-   `numpy`\n",
    "-   `scipy`\n",
    "\n",
    "**System requirements** - A C++ compiler, such as `g++` - Python\n",
    "development package (`python-devel`/`python-dev`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Usage\n",
    "===========\n",
    "\n",
    "We turn on some logging to help us to see what `torch.compile` is doing\n",
    "under the hood in this tutorial. The following code will print out the\n",
    "PyTorch ops that `torch.compile` traced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch._logging.set_logs(graph_code=True)\n",
    "# 不起作用\n",
    "# Enable verbose logging to see compilation details\n",
    "# torch._dynamo.config.verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code] TRACED GRAPH\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]  ===== __compiled_fn_7_9931b1aa_c92a_4185_8547_d7e651d37223 =====\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]     def forward(self, L_x_: \"f32[3, 3][3, 1]cpu\", L_y_: \"f32[3, 3][3, 1]cpu\"):\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         l_x_ = L_x_\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         l_y_ = L_y_\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         \n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         # File: /tmp/ipykernel_197418/14723609.py:2 in foo, code: a = torch.sin(x)\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         a: \"f32[3, 3][3, 1]cpu\" = torch.sin(l_x_);  l_x_ = None\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         \n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         # File: /tmp/ipykernel_197418/14723609.py:3 in foo, code: b = torch.cos(y)\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         b: \"f32[3, 3][3, 1]cpu\" = torch.cos(l_y_);  l_y_ = None\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         \n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         # File: /tmp/ipykernel_197418/14723609.py:4 in foo, code: return a + b\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         add: \"f32[3, 3][3, 1]cpu\" = a + b;  a = b = None\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         return (add,)\n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code]         \n",
      "V0125 15:13:05.402000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/0] [__graph_code] \n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code] TRACED GRAPH\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]  ===== __compiled_fn_9_d958ac78_777e_486b_8702_64f81927b0a2 =====\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]     def forward(self, s77: \"Sym(s17)\", s27: \"Sym(s27)\", L_x_: \"f32[s17, s27][s27, 1]cpu\", L_y_: \"f32[s17, s27][s27, 1]cpu\"):\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         l_x_ = L_x_\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         l_y_ = L_y_\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         \n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         # File: /tmp/ipykernel_197418/14723609.py:2 in foo, code: a = torch.sin(x)\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         a: \"f32[s17, s27][s27, 1]cpu\" = torch.sin(l_x_);  l_x_ = None\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         \n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         # File: /tmp/ipykernel_197418/14723609.py:3 in foo, code: b = torch.cos(y)\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         b: \"f32[s17, s27][s27, 1]cpu\" = torch.cos(l_y_);  l_y_ = None\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         \n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         # File: /tmp/ipykernel_197418/14723609.py:4 in foo, code: return a + b\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         add: \"f32[s17, s27][s27, 1]cpu\" = a + b;  a = b = None\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         return (add,)\n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code]         \n",
      "V0125 15:13:05.465000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [3/1] [__graph_code] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9546,  1.8503,  0.8476],\n",
      "        [-0.0234, -0.1408,  1.9060],\n",
      "        [-1.5669,  0.2384,  1.4532]])\n",
      "tensor([[ 0.0889,  0.9144,  1.0922,  0.3792],\n",
      "        [ 0.8120,  0.0853, -0.2555,  0.7229],\n",
      "        [ 0.2868,  0.0549,  0.6464,  0.5942],\n",
      "        [-0.0562,  0.0907, -0.3454,  0.6626]])\n"
     ]
    }
   ],
   "source": [
    "def foo(x, y):\n",
    "    a = torch.sin(x)\n",
    "    b = torch.cos(y)\n",
    "    return a + b\n",
    "\n",
    "\n",
    "opt_foo1 = torch.compile(foo)\n",
    "print(opt_foo1(torch.randn(3, 3), torch.randn(3, 3)))\n",
    "print(opt_foo1(torch.randn(4, 4), torch.randn(4, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "print(x.shape[0].__class__)  # <class 'int'>\n",
    "print(x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6818, 1.7326, 0.1336],\n",
      "        [0.0587, 0.4029, 0.8624],\n",
      "        [0.7774, 0.9512, 0.2888]])\n",
      "tensor([[ 0.7467,  0.0485,  1.4156,  0.0289],\n",
      "        [ 0.9113,  0.9799,  1.4499,  0.6286],\n",
      "        [ 0.0259, -0.1759, -0.9992, -1.0882],\n",
      "        [ 0.1066,  0.9046, -0.4007,  1.6108]])\n"
     ]
    }
   ],
   "source": [
    "print(opt_foo1(torch.randn(3, 3), torch.randn(3, 3)))\n",
    "print(opt_foo1(torch.randn(4, 4), torch.randn(4, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.compile` is a decorator that takes an arbitrary Python function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code] TRACED GRAPH\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]  ===== __compiled_fn_9_5e62b208_6380_4342_b318_8c393cd6df6f =====\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]     def forward(self, L_x_: \"f32[3, 3][3, 1]cpu\", L_y_: \"f32[3, 3][3, 1]cpu\"):\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         l_x_ = L_x_\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         l_y_ = L_y_\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         \n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         # File: /tmp/ipykernel_179889/4220512634.py:3 in opt_foo2, code: a = torch.sin(x)\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         a: \"f32[3, 3][3, 1]cpu\" = torch.sin(l_x_);  l_x_ = None\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         \n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         # File: /tmp/ipykernel_179889/4220512634.py:4 in opt_foo2, code: b = torch.cos(y)\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         b: \"f32[3, 3][3, 1]cpu\" = torch.cos(l_y_);  l_y_ = None\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         \n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         # File: /tmp/ipykernel_179889/4220512634.py:5 in opt_foo2, code: return a + b\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         add: \"f32[3, 3][3, 1]cpu\" = a + b;  a = b = None\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         return (add,)\n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code]         \n",
      "V0125 14:42:53.693000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [4/0] [__graph_code] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0671,  0.8626,  1.3965],\n",
      "        [ 1.4243,  0.3922,  0.3412],\n",
      "        [-0.9585, -1.1444, -0.2240]])\n"
     ]
    }
   ],
   "source": [
    "@torch.compile\n",
    "def opt_foo2(x, y):\n",
    "    a = torch.sin(x)\n",
    "    b = torch.cos(y)\n",
    "    return a + b\n",
    "\n",
    "print(opt_foo2(torch.randn(3, 3), torch.randn(3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.compile` is applied recursively, so nested function calls within\n",
    "the top-level compiled function will also be compiled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code] TRACED GRAPH\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]  ===== __compiled_fn_13_6f0e438d_f42f_4d18_af2a_f6be1b9424d2 =====\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]     def forward(self, L_x_: \"f32[3, 3][3, 1]cpu\", L_y_: \"f32[3, 3][3, 1]cpu\"):\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         l_x_ = L_x_\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         l_y_ = L_y_\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         \n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         # File: /tmp/ipykernel_179889/2199486826.py:2 in inner, code: return torch.sin(x)\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         a: \"f32[3, 3][3, 1]cpu\" = torch.sin(l_x_);  l_x_ = None\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         \n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         # File: /tmp/ipykernel_179889/2199486826.py:8 in outer, code: b = torch.cos(y)\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         b: \"f32[3, 3][3, 1]cpu\" = torch.cos(l_y_);  l_y_ = None\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         \n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         # File: /tmp/ipykernel_179889/2199486826.py:9 in outer, code: return a + b\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         add: \"f32[3, 3][3, 1]cpu\" = a + b;  a = b = None\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         return (add,)\n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code]         \n",
      "V0125 14:45:58.618000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [6/0] [__graph_code] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7620,  0.5823, -0.2230],\n",
      "        [ 0.4700,  1.6073,  0.1784],\n",
      "        [ 1.5852,  0.1588,  0.6764]])\n"
     ]
    }
   ],
   "source": [
    "def inner(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "\n",
    "@torch.compile\n",
    "def outer(x, y):\n",
    "    a = inner(x)\n",
    "    b = torch.cos(y)\n",
    "    return a + b\n",
    "\n",
    "\n",
    "print(outer(torch.randn(3, 3), torch.randn(3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also optimize `torch.nn.Module` instances by either calling its\n",
    "`.compile()` method or by directly `torch.compile`-ing the module. This\n",
    "is equivalent to `torch.compile`-ing the module\\'s `__call__` method\n",
    "(which indirectly calls `forward`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code] TRACED GRAPH\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]  ===== __compiled_fn_27_9c390f3e_399a_4717_9ceb_a8f78871a303 =====\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]     def forward(self, L_self_modules_lin_parameters_weight_: \"f32[3, 3][3, 1]cpu\", L_self_modules_lin_parameters_bias_: \"f32[3][1]cpu\", L_x_: \"f32[3, 3][3, 1]cpu\"):\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         l_self_modules_lin_parameters_weight_ = L_self_modules_lin_parameters_weight_\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         l_self_modules_lin_parameters_bias_ = L_self_modules_lin_parameters_bias_\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         l_x_ = L_x_\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         \n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         # File: /tmp/ipykernel_179889/3046762702.py:7 in forward, code: return torch.nn.functional.relu(self.lin(x))\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         linear: \"f32[3, 3][3, 1]cpu\" = torch._C._nn.linear(l_x_, l_self_modules_lin_parameters_weight_, l_self_modules_lin_parameters_bias_);  l_x_ = l_self_modules_lin_parameters_weight_ = l_self_modules_lin_parameters_bias_ = None\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         relu: \"f32[3, 3][3, 1]cpu\" = torch.nn.functional.relu(linear);  linear = None\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         return (relu,)\n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code]         \n",
      "V0125 14:51:49.922000 179889 site-packages/torch/_dynamo/output_graph.py:2184] [13/0] [__graph_code] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7187, 0.9459, 0.6886],\n",
      "        [0.0000, 0.5362, 0.3013],\n",
      "        [0.0000, 0.4415, 0.0248]], grad_fn=<CompiledFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = torch.nn.Linear(3, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.relu(self.lin(x))\n",
    "\n",
    "mod2 = MyModule()\n",
    "mod2 = torch.compile(mod2)\n",
    "print(mod2(torch.randn(3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0596, 0.1275],\n",
      "        [0.0000, 0.7664, 0.0000],\n",
      "        [0.1984, 0.2656, 0.0000]], grad_fn=<CompiledFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 一个模型只会被编译一次，重复调用 compile 方法不会重新编译模型\n",
    "# 下面的代码与上面的效果是一样的\n",
    "mod1 = MyModule()\n",
    "mod1.compile()\n",
    "print(mod1(torch.randn(3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrating Speedups\n",
    "======================\n",
    "\n",
    "Now let\\'s demonstrate how `torch.compile` speeds up a simple PyTorch\n",
    "example. For a demonstration on a more complex model, see our\n",
    "[end-to-end torch.compile\n",
    "tutorial](https://pytorch.org/tutorials/intermediate/torch_compile_full_example.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code] TRACED GRAPH\n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code]  ===== __compiled_fn_23_60b27506_864b_4e24_b498_00cccf75ee57 =====\n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code]     def forward(self, L_x_: \"f32[4096, 4096][4096, 1]cuda:0\"):\n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code]         l_x_ = L_x_\n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code]         \n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code]         # File: /tmp/ipykernel_197418/2093821154.py:2 in foo3, code: y = x + 1\n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code]         y: \"f32[4096, 4096][4096, 1]cuda:0\" = l_x_ + 1;  l_x_ = None\n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code]         \n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code]         # File: /tmp/ipykernel_197418/2093821154.py:3 in foo3, code: z = torch.nn.functional.relu(y)\n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code]         z: \"f32[4096, 4096][4096, 1]cuda:0\" = torch.nn.functional.relu(y);  y = None\n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code]         \n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code]         # File: /tmp/ipykernel_197418/2093821154.py:4 in foo3, code: u = z * 2\n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code]         u: \"f32[4096, 4096][4096, 1]cuda:0\" = z * 2;  z = None\n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code]         return (u,)\n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code]         \n",
      "V0125 16:20:30.437000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [10/0] [__graph_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] Output code: \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] # AOT ID: ['14_inference']\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] from ctypes import c_void_p, c_long, c_int\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] import torch\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] import math\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] import random\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] import os\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] import tempfile\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] from math import inf, nan\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] from cmath import nanj\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] from torch._inductor.utils import maybe_profile\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] from torch import device, empty_strided\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] from torch._inductor.async_compile import AsyncCompile\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] import triton\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] import triton.language as tl\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] aten = torch.ops.aten\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] inductor_ops = torch.ops.inductor\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] _quantized = torch.ops._quantized\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] async_compile = AsyncCompile()\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] # kernel path: /tmp/torchinductor_hliu/z3/cz3s7v375snhwxwgnls2kvylx32ytpvubpicl6tp5uidlbf76p55.py\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] # Topologically Sorted Source Nodes: [y, z, u], Original ATen: [aten.add, aten.relu, aten.mul]\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] # Source node to ATen node mapping:\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] #   u => mul\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] #   y => add\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] #   z => relu\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] # Graph fragment:\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] #   %arg0_1 : Tensor \"f32[4096, 4096][4096, 1]cuda:0\" = PlaceHolder[target=arg0_1]\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] #   %add : Tensor \"f32[4096, 4096][4096, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg0_1, 1), kwargs = {})\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] #   %relu : Tensor \"f32[4096, 4096][4096, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%add,), kwargs = {})\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] #   %mul : Tensor \"f32[4096, 4096][4096, 1]cuda:0\"[num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%relu, 2), kwargs = {})\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] #   return %mul\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] triton_poi_fused_add_mul_relu_0 = async_compile.triton('triton_poi_fused_add_mul_relu_0', '''\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] import triton\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] import triton.language as tl\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] triton_helpers.set_driver_to_gpu()\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] @triton_heuristics.pointwise(\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     size_hints={'x': 16777216}, \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     filename=__file__,\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=38, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, max_threads_per_block=1024, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_mul_relu_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 1, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'E057B5C8BF10B767FB47346465DC415F506334ED3AA872FE1C8996DCA442A2CF', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 201326592}},\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     min_elem_per_thread=0\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] )\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] @triton.jit\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] def triton_poi_fused_add_mul_relu_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     xnumel = 16777216\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     x0 = xindex\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x0), None)\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     tmp1 = 1.0\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     tmp2 = tmp0 + tmp1\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     tmp3 = tl.full([1], 0, tl.int32)\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     tmp4 = triton_helpers.maximum(tmp3, tmp2)\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     tmp5 = 2.0\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     tmp6 = tmp4 * tmp5\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     tl.store(out_ptr0 + (x0), tmp6, None)\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] ''', device_str='cuda')\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] async_compile.wait(globals())\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] del async_compile\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] class Runner:\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     def __init__(self, partitions):\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]         self.partitions = partitions\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     def recursively_apply_fns(self, fns):\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]         new_callables = []\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]         for fn, c in zip(fns, self.partitions):\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]             new_callables.append(fn(c))\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]         self.partitions = new_callables\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     def call(self, args):\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]         arg0_1, = args\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]         args.clear()\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]         assert_size_stride(arg0_1, (4096, 4096), (4096, 1))\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]         with torch.cuda._DeviceGuard(0):\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]             torch.cuda.set_device(0)\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]             buf0 = empty_strided_cuda((4096, 4096), (4096, 1), torch.float32)\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]             # Topologically Sorted Source Nodes: [y, z, u], Original ATen: [aten.add, aten.relu, aten.mul]\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]             stream0 = get_raw_stream(0)\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]             triton_poi_fused_add_mul_relu_0.run(arg0_1, buf0, 16777216, stream=stream0)\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]             del arg0_1\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]         return (buf0, )\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] runner = Runner(partitions=[])\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] call = runner.call\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     from torch._dynamo.testing import rand_strided\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     from torch._inductor.utils import print_performance\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     arg0_1 = rand_strided((4096, 4096), (4096, 1), device='cuda:0', dtype=torch.float32)\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     fn = lambda: call([arg0_1])\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] if __name__ == \"__main__\":\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)\n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1250] [10/0] [__output_code] \n",
      "V0125 16:20:30.445000 197418 site-packages/torch/_inductor/codecache.py:1251] [10/0] [__output_code] Output code written to: /tmp/torchinductor_hliu/ik/cikq4iqgzv6r3sfscudag4xkvyud6xzdlho4tsfgyebebnhyuigl.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile: 0.02723142433166504\n",
      "eager: 0.0008028159737586975\n"
     ]
    }
   ],
   "source": [
    "def foo3(x):\n",
    "    y = x + 1\n",
    "    z = torch.nn.functional.relu(y)\n",
    "    u = z * 2\n",
    "    return u\n",
    "\n",
    "# 启用 Inductor 生成的底层代码日志\n",
    "torch._logging.set_logs(graph_code=True, output_code=True)\n",
    "opt_foo3 = torch.compile(foo3)\n",
    "\n",
    "\n",
    "# Returns the result of running `fn()` and the time it took for `fn()` to run,\n",
    "# in seconds. We use CUDA events and synchronization for the most accurate\n",
    "# measurements.\n",
    "def timed(fn):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end) / 1000\n",
    "\n",
    "# Warmup # 运行一次以触发编译\n",
    "inp = torch.randn(4096, 4096).cuda()\n",
    "print(\"compile:\", timed(lambda: opt_foo3(inp))[1])\n",
    "print(\"eager:\", timed(lambda: foo3(inp))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile: 0.00048732799291610715\n",
      "eager: 0.0009031040072441101\n"
     ]
    }
   ],
   "source": [
    "inp = torch.randn(4096, 4096).cuda()\n",
    "# why Faster?\n",
    "# @triton.jit\n",
    "# def triton_poi_fused_add_mul_relu_0\n",
    "print(\"compile:\", timed(lambda: opt_foo3(inp))[1])\n",
    "print(\"eager:\", timed(lambda: foo3(inp))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `torch.compile` appears to take a lot longer to complete\n",
    "compared to eager. This is because `torch.compile` takes extra time to\n",
    "compile the model on the first few executions. `torch.compile` re-uses\n",
    "compiled code whever possible, so if we run our optimized model several\n",
    "more times, we should see a significant improvement compared to eager.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager time 0: 0.0008069120049476623\n",
      "eager time 1: 0.0007383040189743042\n",
      "eager time 2: 0.000735647976398468\n",
      "eager time 3: 0.000735584020614624\n",
      "eager time 4: 0.0007297919988632203\n",
      "eager time 5: 0.0007301120162010193\n",
      "eager time 6: 0.000729088008403778\n",
      "eager time 7: 0.0007279679775238037\n",
      "eager time 8: 0.0007321599721908569\n",
      "eager time 9: 0.0007331839799880981\n",
      "~~~~~~~~~~\n",
      "compile time 0: 0.000374783992767334\n",
      "compile time 1: 0.0002744320034980774\n",
      "compile time 2: 0.0002652159929275513\n",
      "compile time 3: 0.0002631680071353912\n",
      "compile time 4: 0.00026214399933815005\n",
      "compile time 5: 0.0002611199915409088\n",
      "compile time 6: 0.0002682879865169525\n",
      "compile time 7: 0.00026624000072479246\n",
      "compile time 8: 0.00028569599986076356\n",
      "compile time 9: 0.0002703680098056793\n",
      "~~~~~~~~~~\n",
      "(eval) eager median: 0.0007326719760894775, compile median: 0.0002672639936208725, speedup: 2.74137928631273x\n",
      "~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "# turn off logging for now to prevent spam\n",
    "torch._logging.set_logs(graph_code=False, output_code=False)\n",
    "\n",
    "eager_times = []\n",
    "for i in range(10):\n",
    "    _, eager_time = timed(lambda: foo3(inp))\n",
    "    eager_times.append(eager_time)\n",
    "    print(f\"eager time {i}: {eager_time}\")\n",
    "print(\"~\" * 10)\n",
    "\n",
    "compile_times = []\n",
    "for i in range(10):\n",
    "    _, compile_time = timed(lambda: opt_foo3(inp))\n",
    "    compile_times.append(compile_time)\n",
    "    print(f\"compile time {i}: {compile_time}\")\n",
    "print(\"~\" * 10)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "eager_med = np.median(eager_times)\n",
    "compile_med = np.median(compile_times)\n",
    "speedup = eager_med / compile_med\n",
    "assert speedup > 1\n",
    "print(\n",
    "    f\"(eval) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\"\n",
    ")\n",
    "print(\"~\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, we can see that running our model with `torch.compile`\n",
    "results in a significant speedup. Speedup mainly comes from reducing\n",
    "Python overhead and GPU read/writes, and so the observed speedup may\n",
    "vary on factors such as model architecture and batch size. For example,\n",
    "if a model\\'s architecture is simple and the amount of data is large,\n",
    "then the bottleneck would be GPU compute and the observed speedup may be\n",
    "less significant.\n",
    "\n",
    "To see speedups on a real model, check out our [end-to-end torch.compile\n",
    "tutorial](https://pytorch.org/tutorials/intermediate/torch_compile_full_example.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benefits over TorchScript\n",
    "=========================\n",
    "\n",
    "Why should we use `torch.compile` over TorchScript? Primarily, the\n",
    "advantage of `torch.compile` lies in its ability to handle arbitrary\n",
    "Python code with minimal changes to existing code.\n",
    "\n",
    "Compare to TorchScript, which has a tracing mode (`torch.jit.trace`) and\n",
    "a scripting mode (`torch.jit.script`). Tracing mode is susceptible to\n",
    "silent incorrectness, while scripting mode requires significant code\n",
    "changes and will raise errors on unsupported Python code.\n",
    "\n",
    "For example, TorchScript tracing silently fails on data-dependent\n",
    "control flow (the `if x.sum() < 0:` line below) because only the actual\n",
    "control flow path is traced. In comparison, `torch.compile` is able to\n",
    "correctly handle it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traced 1, 1: True\n",
      "traced 1, 2: False\n",
      "compile 1, 1: True\n",
      "compile 1, 2: True\n",
      "~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_197418/65032246.py:2: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.sum() < 0:\n"
     ]
    }
   ],
   "source": [
    "def f1(x, y):\n",
    "    if x.sum() < 0:\n",
    "        return -y\n",
    "    return y\n",
    "\n",
    "\n",
    "# Test that `fn1` and `fn2` return the same result, given the same arguments `args`.\n",
    "def test_fns(fn1, fn2, args):\n",
    "    out1 = fn1(*args)\n",
    "    out2 = fn2(*args)\n",
    "    return torch.allclose(out1, out2)\n",
    "\n",
    "\n",
    "inp1 = torch.randn(5, 5)\n",
    "inp2 = torch.randn(5, 5)\n",
    "\n",
    "traced_f1 = torch.jit.trace(f1, (inp1, inp2))\n",
    "print(\"traced 1, 1:\", test_fns(f1, traced_f1, (inp1, inp2)))\n",
    "print(\"traced 1, 2:\", test_fns(f1, traced_f1, (-inp1, inp2)))\n",
    "\n",
    "compile_f1 = torch.compile(f1)\n",
    "print(\"compile 1, 1:\", test_fns(f1, compile_f1, (inp1, inp2)))\n",
    "print(\"compile 1, 2:\", test_fns(f1, compile_f1, (-inp1, inp2)))\n",
    "print(\"~\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchScript scripting can handle data-dependent control flow, but it can\n",
    "require major code changes and will raise errors when unsupported Python\n",
    "is used.\n",
    "\n",
    "In the example below, we forget TorchScript type annotations and we\n",
    "receive a TorchScript error because the input type for argument `y`, an\n",
    "`int`, does not match with the default argument type, `torch.Tensor`. In\n",
    "comparison, `torch.compile` works without requiring any type\n",
    "annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_197418/3652677659.py\", line 15, in <module>\n",
      "    script_f2(inp1, inp2)\n",
      "    ~~~~~~~~~^^^^^^^^^^^^\n",
      "RuntimeError: f2() Expected a value of type 'Tensor (inferred)' for argument 'y' but instead found type 'int'.\n",
      "Inferred 'y' to be of type 'Tensor' because it was not annotated with an explicit type.\n",
      "Position: 1\n",
      "Value: 3\n",
      "Declaration: f2(Tensor x, Tensor y) -> Tensor\n",
      "Cast error details: Unable to cast 3 to Tensor\n",
      "V0125 16:42:57.616000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [19/0] [__graph_code] TRACED GRAPH\n",
      "V0125 16:42:57.616000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [19/0] [__graph_code]  ===== __compiled_fn_43_15c554a6_e2c3_4bf5_a206_0da60a135e49 =====\n",
      "V0125 16:42:57.616000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [19/0] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 16:42:57.616000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [19/0] [__graph_code]     def forward(self, L_x_: \"f32[5, 5][5, 1]cpu\"):\n",
      "V0125 16:42:57.616000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [19/0] [__graph_code]         l_x_ = L_x_\n",
      "V0125 16:42:57.616000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [19/0] [__graph_code]         \n",
      "V0125 16:42:57.616000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [19/0] [__graph_code]         # File: /tmp/ipykernel_197418/3652677659.py:7 in f2, code: return x + y\n",
      "V0125 16:42:57.616000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [19/0] [__graph_code]         add: \"f32[5, 5][5, 1]cpu\" = l_x_ + 3;  l_x_ = None\n",
      "V0125 16:42:57.616000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [19/0] [__graph_code]         return (add,)\n",
      "V0125 16:42:57.616000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [19/0] [__graph_code]         \n",
      "V0125 16:42:57.616000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [19/0] [__graph_code] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile 2: True\n",
      "~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "import traceback as tb\n",
    "\n",
    "torch._logging.set_logs(graph_code=True)\n",
    "\n",
    "\n",
    "def f2(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "inp1 = torch.randn(5, 5)\n",
    "inp2 = 3\n",
    "\n",
    "script_f2 = torch.jit.script(f2)\n",
    "try:\n",
    "    script_f2(inp1, inp2)\n",
    "except:\n",
    "    tb.print_exc()\n",
    "\n",
    "compile_f2 = torch.compile(f2)\n",
    "print(\"compile 2:\", test_fns(f2, compile_f2, (inp1, inp2)))\n",
    "print(\"~\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Breaks\n",
    "============\n",
    "\n",
    "The graph break is one of the most fundamental concepts within\n",
    "`torch.compile`. It allows `torch.compile` to handle arbitrary Python\n",
    "code by interrupting compilation, running the unsupported code, then\n",
    "resuming compilation. The term \\\"graph break\\\" comes from the fact that\n",
    "`torch.compile` attempts to capture and optimize the PyTorch operation\n",
    "graph. When unsupported Python code is encountered, then this graph must\n",
    "be \\\"broken\\\". Graph breaks result in lost optimization opportunities,\n",
    "which may still be undesirable, but this is better than silent\n",
    "incorrectness or a hard crash.\n",
    "\n",
    "Let\\'s look at a data-dependent control flow example to better see how\n",
    "graph breaks work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code] TRACED GRAPH\n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]  ===== __compiled_fn_45_196dda0d_23c7_4766_b135_c5a1aebde606 =====\n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]     def forward(self, L_a_: \"f32[10][1]cpu\", L_b_: \"f32[10][1]cpu\"):\n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]         l_a_ = L_a_\n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]         l_b_ = L_b_\n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]         \n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]         # File: /tmp/ipykernel_197418/49326488.py:2 in bar, code: x = a / (torch.abs(a) + 1)\n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]         abs_1: \"f32[10][1]cpu\" = torch.abs(l_a_)\n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]         add: \"f32[10][1]cpu\" = abs_1 + 1;  abs_1 = None\n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]         x: \"f32[10][1]cpu\" = l_a_ / add;  l_a_ = add = None\n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]         \n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]         # File: /tmp/ipykernel_197418/49326488.py:3 in bar, code: if b.sum() < 0:\n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]         sum_1: \"f32[][]cpu\" = l_b_.sum();  l_b_ = None\n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]         lt: \"b8[][]cpu\" = sum_1 < 0;  sum_1 = None\n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]         return (lt, x)\n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code]         \n",
      "V0125 16:56:29.807000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [20/0] [__graph_code] \n",
      "V0125 16:56:30.159000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [21/0] [__graph_code] TRACED GRAPH\n",
      "V0125 16:56:30.159000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [21/0] [__graph_code]  ===== __compiled_fn_49_0ba896b0_f174_4d2d_991f_58dede14ca67 =====\n",
      "V0125 16:56:30.159000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [21/0] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 16:56:30.159000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [21/0] [__graph_code]     def forward(self, L_x_: \"f32[10][1]cpu\", L_b_: \"f32[10][1]cpu\"):\n",
      "V0125 16:56:30.159000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [21/0] [__graph_code]         l_x_ = L_x_\n",
      "V0125 16:56:30.159000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [21/0] [__graph_code]         l_b_ = L_b_\n",
      "V0125 16:56:30.159000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [21/0] [__graph_code]         \n",
      "V0125 16:56:30.159000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [21/0] [__graph_code]         # File: /tmp/ipykernel_197418/49326488.py:5 in torch_dynamo_resume_in_bar_at_3, code: return x * b\n",
      "V0125 16:56:30.159000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [21/0] [__graph_code]         mul: \"f32[10][1]cpu\" = l_x_ * l_b_;  l_x_ = l_b_ = None\n",
      "V0125 16:56:30.159000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [21/0] [__graph_code]         return (mul,)\n",
      "V0125 16:56:30.159000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [21/0] [__graph_code]         \n",
      "V0125 16:56:30.159000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [21/0] [__graph_code] \n",
      "V0125 16:56:30.464000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [22/0] [__graph_code] TRACED GRAPH\n",
      "V0125 16:56:30.464000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [22/0] [__graph_code]  ===== __compiled_fn_51_c99debc4_919b_4377_90a8_39227567291a =====\n",
      "V0125 16:56:30.464000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [22/0] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 16:56:30.464000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [22/0] [__graph_code]     def forward(self, L_b_: \"f32[10][1]cpu\", L_x_: \"f32[10][1]cpu\"):\n",
      "V0125 16:56:30.464000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [22/0] [__graph_code]         l_b_ = L_b_\n",
      "V0125 16:56:30.464000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [22/0] [__graph_code]         l_x_ = L_x_\n",
      "V0125 16:56:30.464000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [22/0] [__graph_code]         \n",
      "V0125 16:56:30.464000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [22/0] [__graph_code]         # File: /tmp/ipykernel_197418/49326488.py:4 in torch_dynamo_resume_in_bar_at_3, code: b = b * -1\n",
      "V0125 16:56:30.464000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [22/0] [__graph_code]         b: \"f32[10][1]cpu\" = l_b_ * -1;  l_b_ = None\n",
      "V0125 16:56:30.464000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [22/0] [__graph_code]         \n",
      "V0125 16:56:30.464000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [22/0] [__graph_code]         # File: /tmp/ipykernel_197418/49326488.py:5 in torch_dynamo_resume_in_bar_at_3, code: return x * b\n",
      "V0125 16:56:30.464000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [22/0] [__graph_code]         mul_1: \"f32[10][1]cpu\" = l_x_ * b;  l_x_ = b = None\n",
      "V0125 16:56:30.464000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [22/0] [__graph_code]         return (mul_1,)\n",
      "V0125 16:56:30.464000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [22/0] [__graph_code]         \n",
      "V0125 16:56:30.464000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [22/0] [__graph_code] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "        0.5000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bar(a, b):\n",
    "    x = a / (torch.abs(a) + 1)\n",
    "    if b.sum() < 0:\n",
    "        b = b * -1\n",
    "    return x * b\n",
    "\n",
    "\n",
    "opt_bar = torch.compile(bar)\n",
    "inp1 = torch.ones(10)\n",
    "inp2 = torch.ones(10)\n",
    "opt_bar(inp1, inp2)\n",
    "opt_bar(inp1, -inp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time we run `bar`, we see that `torch.compile` traced 2 graphs\n",
    "corresponding to the following code (noting that `b.sum() < 0` is\n",
    "False):\n",
    "\n",
    "1.  `x = a / (torch.abs(a) + 1); b.sum()`\n",
    "2.  `return x * b`\n",
    "\n",
    "The second time we run `bar`, we take the other branch of the if\n",
    "statement and we get 1 traced graph corresponding to the code\n",
    "`b = b * -1; return x * b`. We do not see a graph of\n",
    "`x = a / (torch.abs(a) + 1)` outputted the second time since\n",
    "`torch.compile` cached this graph from the first run and re-used it.\n",
    "\n",
    "Let\\'s investigate by example how TorchDynamo would step through `bar`.\n",
    "If `b.sum() < 0`, then TorchDynamo would run graph 1, let Python\n",
    "determine the result of the conditional, then run graph 2. On the other\n",
    "hand, if `not b.sum() < 0`, then TorchDynamo would run graph 1, let\n",
    "Python determine the result of the conditional, then run graph 3.\n",
    "\n",
    "We can see all graph breaks by using\n",
    "`torch._logging.set_logs(graph_breaks=True)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] Graph break in user code at /tmp/ipykernel_197418/49326488.py:3\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] Graph Break Reason: Data-dependent branching\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   Explanation: Detected data-dependent branching (e.g. `if my_tensor.sum() > 0:`). Dynamo does not support tracing dynamic control flow.\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   Hint: Use `torch.cond` to express dynamic control flow.\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] \n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   Developer debug context: attempted to jump with TensorVariable()\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] \n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] User code traceback:\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"<frozen runpy>\", line 88, in _run_code\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     app.launch_new_instance()\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     app.start()\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     self.io_loop.start()\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     self.asyncio_loop.run_forever()\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/asyncio/base_events.py\", line 683, in run_forever\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     self._run_once()\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/asyncio/base_events.py\", line 2050, in _run_once\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     handle._run()\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/asyncio/events.py\", line 89, in _run\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     self._context.run(self._callback, *self._args)\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     await self.process_one()\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     await dispatch(*args)\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     await result\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     await super().execute_request(stream, ident, parent)\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     reply_content = await reply_content\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     res = shell.run_cell(\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/ipykernel/zmqshell.py\", line 602, in run_cell\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     return super().run_cell(*args, **kwargs)\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     result = self._run_cell(\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     result = runner(coro)\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     coro.send(None)\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     if await self.run_code(code, result, async_=asy):\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/home/hliu/anaconda3/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/tmp/ipykernel_197418/1712397169.py\", line 4, in <module>\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     opt_bar(inp1, inp2)\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] \n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] ========== most recent `torch.compile` tracing attempt started here ==========\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] \n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]   File \"/tmp/ipykernel_197418/49326488.py\", line 3, in bar\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks]     if b.sum() < 0:\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] \n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] NOTE: the most recent `torch.compile` tracing attempt might not be where you applied `torch.compile`! This is due to how graph breaks are implemented - the optimized code object returned by Dynamo will call another Dynamo-generated resume function and tracing is re-enabled by calling the resume function as a normal Python function, which Dynamo intercepts as a top-level frame.\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] \n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] Most recent bytecode instructions traced (max 20):\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE RESUME 0 []\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE LOAD_FAST 'a' []\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE LOAD_GLOBAL 'torch' [LazyVariableTracker(unrealized: <class 'torch.Tensor'>)]\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE LOAD_ATTR 'abs' [LazyVariableTracker(unrealized: <class 'torch.Tensor'>), LazyVariableTracker(unrealized: <class 'module'>)]\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE LOAD_FAST 'a' [LazyVariableTracker(unrealized: <class 'torch.Tensor'>), LazyVariableTracker(unrealized: <class 'builtin_function_or_method'>), NullVariable]\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE CALL 1 [LazyVariableTracker(unrealized: <class 'torch.Tensor'>), LazyVariableTracker(unrealized: <class 'builtin_function_or_method'>), NullVariable, LazyVariableTracker(unrealized: <class 'torch.Tensor'>)]\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE LOAD_CONST 1 [LazyVariableTracker(realized: TensorVariable()), TensorVariable()]\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE BINARY_OP 0 [LazyVariableTracker(realized: TensorVariable()), TensorVariable(), ConstantVariable(int: 1)]\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE BINARY_OP 11 [LazyVariableTracker(realized: TensorVariable()), TensorVariable()]\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE STORE_FAST 'x' [TensorVariable()]\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE LOAD_FAST 'b' []\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE LOAD_ATTR 'sum' [LazyVariableTracker(unrealized: <class 'torch.Tensor'>)]\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE CALL 0 [GetAttrVariable(TensorVariable(), sum), NullVariable]\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE LOAD_CONST 0 [TensorVariable()]\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE COMPARE_OP '<' [TensorVariable(), ConstantVariable(int: 0)]\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] TRACE POP_JUMP_IF_FALSE 106 [TensorVariable()]\n",
      "V0125 17:04:01.233000 197418 site-packages/torch/_dynamo/symbolic_convert.py:4275] [0/0] [__graph_breaks] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "        0.5000])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch._logging.set_logs(graph_breaks=True)\n",
    "# Reset to clear the torch.compile cache\n",
    "torch._dynamo.reset()\n",
    "opt_bar(inp1, inp2)\n",
    "opt_bar(inp1, -inp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to maximize speedup, graph breaks should be limited. We can\n",
    "force TorchDynamo to raise an error upon the first graph break\n",
    "encountered by using `fullgraph=True`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_197418/387069252.py\", line 6, in <module>\n",
      "    opt_bar_fullgraph(torch.randn(10), torch.randn(10))\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py\", line 953, in compile_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py\", line 2202, in __call__\n",
      "    result = self._torchdynamo_orig_backend(\n",
      "        frame, cache_entry, self.hooks, frame_state, skip=1\n",
      "    )\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py\", line 707, in __call__\n",
      "    result = _compile(\n",
      "        frame.f_code,\n",
      "    ...<16 lines>...\n",
      "        convert_frame_box=self._box,\n",
      "    )\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py\", line 1752, in _compile\n",
      "    guarded_code, tracer_output = compile_inner(code, one_graph, hooks)\n",
      "                                  ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_utils_internal.py\", line 97, in wrapper_function\n",
      "    return function(*args, **kwargs)\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py\", line 1433, in compile_inner\n",
      "    return _compile_inner(code, one_graph, hooks)\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py\", line 1467, in _compile_inner\n",
      "    dynamo_output = compile_frame(\n",
      "        code,\n",
      "    ...<11 lines>...\n",
      "        package=package,\n",
      "    )\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py\", line 1341, in compile_frame\n",
      "    bytecode, tracer_output = transform_code_object(code, transform)\n",
      "                              ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1600, in transform_code_object\n",
      "    tracer_output = transformations(instructions, code_options)\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py\", line 1313, in transform\n",
      "    tracer_output = trace_frame(\n",
      "        code,\n",
      "    ...<14 lines>...\n",
      "        package=package,\n",
      "    )\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py\", line 328, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py\", line 838, in trace_frame\n",
      "    run_tracer()\n",
      "    ~~~~~~~~~~^^\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/convert_frame.py\", line 819, in run_tracer\n",
      "    tracer.run()\n",
      "    ~~~~~~~~~~^^\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/symbolic_convert.py\", line 1654, in run\n",
      "    while self.step():\n",
      "          ~~~~~~~~~^^\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/symbolic_convert.py\", line 1334, in step\n",
      "    self.dispatch_table[inst.opcode](self, inst)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/symbolic_convert.py\", line 830, in inner\n",
      "    unimplemented(\n",
      "    ~~~~~~~~~~~~~^\n",
      "        gb_type=\"Data-dependent branching\",\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        ],\n",
      "        ^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/hliu/anaconda3/lib/python3.13/site-packages/torch/_dynamo/exc.py\", line 588, in unimplemented\n",
      "    raise Unsupported(msg)\n",
      "torch._dynamo.exc.Unsupported: Data-dependent branching\n",
      "  Explanation: Detected data-dependent branching (e.g. `if my_tensor.sum() > 0:`). Dynamo does not support tracing dynamic control flow.\n",
      "  Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.\n",
      "  Hint: Use `torch.cond` to express dynamic control flow.\n",
      "\n",
      "  Developer debug context: attempted to jump with TensorVariable()\n",
      "\n",
      " For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0170.html\n",
      "\n",
      "from user code:\n",
      "   File \"/tmp/ipykernel_197418/49326488.py\", line 3, in bar\n",
      "    if b.sum() < 0:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reset to clear the torch.compile cache\n",
    "torch._dynamo.reset()\n",
    "\n",
    "opt_bar_fullgraph = torch.compile(bar, fullgraph=True)\n",
    "try:\n",
    "    opt_bar_fullgraph(torch.randn(10), torch.randn(10))\n",
    "except:\n",
    "    tb.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example above, we can work around this graph break by replacing\n",
    "the if statement with a `torch.cond`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code] TRACED GRAPH\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]  ===== __compiled_fn_64_8ac9cba9_53bc_47ee_b0fe_8854e6cc4561 =====\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]  /home/hliu/anaconda3/lib/python3.13/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]     def forward(self, L_a_: \"f32[10][1]cpu\", L_b_: \"f32[10][1]cpu\"):\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         l_a_ = L_a_\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         l_b_ = L_b_\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         \n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         # File: /tmp/ipykernel_197418/2155638989.py:7 in bar_fixed, code: x = a / (torch.abs(a) + 1)\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         abs_1: \"f32[10][1]cpu\" = torch.abs(l_a_)\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         add: \"f32[10][1]cpu\" = abs_1 + 1;  abs_1 = None\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         x: \"f32[10][1]cpu\" = l_a_ / add;  l_a_ = add = x = None\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         \n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         # File: /tmp/ipykernel_197418/2155638989.py:16 in bar_fixed, code: x = cond(b.sum() < 0, true_branch, false_branch, (b,))\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         sum_1: \"f32[][]cpu\" = l_b_.sum()\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         lt: \"b8[][]cpu\" = sum_1 < 0;  sum_1 = None\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         \n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         # File: /home/hliu/anaconda3/lib/python3.13/site-packages/torch/_higher_order_ops/cond.py:170 in cond, code: return cond_op(pred, true_fn, false_fn, operands)\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         cond_true_0 = self.cond_true_0\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         cond_false_0 = self.cond_false_0\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         cond = torch.ops.higher_order.cond(lt, cond_true_0, cond_false_0, (l_b_,));  lt = cond_true_0 = cond_false_0 = None\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         x_1: \"f32[10][1]cpu\" = cond[0];  cond = None\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         \n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         # File: /tmp/ipykernel_197418/2155638989.py:17 in bar_fixed, code: return x * b\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         mul: \"f32[10][1]cpu\" = x_1 * l_b_;  x_1 = l_b_ = None\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         return (mul,)\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         \n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]     class cond_true_0(torch.nn.Module):\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         def forward(self, l_b_: \"f32[10][1]cpu\"):\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]             l_b__1 = l_b_\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]             \n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]             # File: /tmp/ipykernel_197418/2155638989.py:10 in true_branch, code: return y * -1\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]             mul: \"f32[10][1]cpu\" = l_b__1 * -1;  l_b__1 = None\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]             return (mul,)\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]             \n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]     class cond_false_0(torch.nn.Module):\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]         def forward(self, l_b_: \"f32[10][1]cpu\"):\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]             l_b__1 = l_b_\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]             \n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]             # File: /tmp/ipykernel_197418/2155638989.py:14 in false_branch, code: return y.clone()\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]             clone: \"f32[10][1]cpu\" = l_b__1.clone();  l_b__1 = None\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]             return (clone,)\n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code]             \n",
      "V0125 17:11:05.755000 197418 site-packages/torch/_dynamo/output_graph.py:2184] [2/0] [__graph_code] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functorch.experimental.control_flow import cond\n",
    "\n",
    "torch._logging.set_logs(graph_code=True)\n",
    "\n",
    "@torch.compile(fullgraph=True)\n",
    "def bar_fixed(a, b):\n",
    "    x = a / (torch.abs(a) + 1)\n",
    "\n",
    "    def true_branch(y):\n",
    "        return y * -1\n",
    "\n",
    "    def false_branch(y):\n",
    "        # NOTE: torch.cond doesn't allow aliased outputs\n",
    "        return y.clone()\n",
    "\n",
    "    x = cond(b.sum() < 0, true_branch, false_branch, (b,))\n",
    "    return x * b\n",
    "\n",
    "\n",
    "bar_fixed(inp1, inp2)\n",
    "bar_fixed(inp1, -inp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to serialize graphs or to run graphs on different (i.e.\n",
    "Python-less) environments, consider using `torch.export` instead (from\n",
    "PyTorch 2.1+). One important restriction is that `torch.export` does not\n",
    "support graph breaks. Please check [the torch.export\n",
    "tutorial](https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html)\n",
    "for more details on `torch.export`.\n",
    "\n",
    "Check out our [section on graph breaks in the torch.compile programming\n",
    "model](https://docs.pytorch.org/docs/main/compile/programming_model.graph_breaks_index.html)\n",
    "for tips on how to work around graph breaks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troubleshooting\n",
    "===============\n",
    "\n",
    "Is `torch.compile` failing to speed up your model? Is compile time\n",
    "unreasonably long? Is your code recompiling excessively? Are you having\n",
    "difficulties dealing with graph breaks? Are you looking for tips on how\n",
    "to best use `torch.compile`? Or maybe you simply want to learn more\n",
    "about the inner workings of `torch.compile`?\n",
    "\n",
    "Check out [the torch.compile programming\n",
    "model](https://docs.pytorch.org/docs/main/compile/programming_model.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "==========\n",
    "\n",
    "In this tutorial, we introduced `torch.compile` by covering basic usage,\n",
    "demonstrating speedups over eager mode, comparing to TorchScript, and\n",
    "briefly describing graph breaks.\n",
    "\n",
    "For an end-to-end example on a real model, check out our [end-to-end\n",
    "torch.compile\n",
    "tutorial](https://pytorch.org/tutorials/intermediate/torch_compile_full_example.html).\n",
    "\n",
    "To troubleshoot issues and to gain a deeper understanding of how to\n",
    "apply `torch.compile` to your code, check out [the torch.compile\n",
    "programming\n",
    "model](https://docs.pytorch.org/docs/main/compile/programming_model.html).\n",
    "\n",
    "We hope that you will give `torch.compile` a try!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
